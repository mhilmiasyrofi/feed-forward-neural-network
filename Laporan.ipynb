{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar II\n",
    "## Feed Forward Neural Network\n",
    "### IF4071 Pembelajaran Mesin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Implementasi Classifier from Scratch\n",
    "\n",
    "Dibuat Neural Network untuk melakukan klasifikasi data weather. Neural Network merupakan fully connected layer yang memiliki jumlah hidden layer maksimal 10. Jumlah node dalam setiap hidden layer dapat bervariasi. Bagian backpropagation diimplementasikan  seperti contoh algoritma pada buku Tom Mitchell hal. 98. Neural network menggunakan fungsi aktivasi sigmoid untuk semua hidden layer maupun output layer. Node output untuk klasifikasi berjumlah 1. \n",
    "\n",
    "Program memberikan pilihan untuk menggunakan momentum atau tidak. Program mengimplementasikan mini-batch stochastic gradient descent. Prorgram Stokasti Gradien Descent diimplementasikan jenis incremental (batch-size=1) dan jenis batch (batch-size=jumlah data).\n",
    "\n",
    "Fungsi loss yang digunakan pada program yang diimplementasikan kali ini adalah MSE, yaitu:\n",
    "![MSE Lossunction](img/mse.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "~~~~~~~~~~\n",
    "Sebuah kelas yang mengimplementasikan SGD untuk \n",
    "sebuah feed forward neural network.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"List ``sizes`` berisi jumlah neuron sesuai dengan\n",
    "        urutan layer. Sebagai contoh, jika dimasukkan list [2, 3, 1]\n",
    "        maka akan digenerate 3 layer network dengan layer input berisi\n",
    "        2 neuron, hiddel layer berisi 3 neuron dan layer output 1 neuron.\n",
    "        Bias dan weight diinisialisasi secara random, menggunakan \n",
    "        distribusi Gaussian dengan mean 0, dan variance 1.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.print_status = False\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Melakukan feed forward dengan input ``a``\"\"\"\n",
    "        activation = a\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b.transpose()[0]\n",
    "            activation = sigmoid(z)\n",
    "        return activation\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, learning_rate, \n",
    "            momentum=0,test_data=None, print_status=False):\n",
    "        \"\"\"Melatih NN dengan mini-batch SGD.\"\"\"\n",
    "        self.print_status = print_status\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            prev_weights = None\n",
    "            prev_biases = None\n",
    "            first = True\n",
    "            for mini_batch in mini_batches:\n",
    "                if first :\n",
    "                    prev_weights = self.weights\n",
    "                    prev_biases = self.biases\n",
    "                prev_weights, prev_biases = self.update_mini_batch(mini_batch, learning_rate, momentum, prev_weights, prev_biases)\n",
    "    #             if self.print_status :\n",
    "            if test_data:\n",
    "                print(\"Epoch \" +  str(j+1))\n",
    "                print(\"\\tAccuracy: \" + str(100*self.evaluate(test_data)/n_test) + \"%\")\n",
    "            else:\n",
    "                print(\"Epoch \" + str(j+1) + \" complete\")\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, learning_rate, momentum, prev_weights, prev_biases):\n",
    "        \"\"\"Update bobot dari network dengan mengaplikasikan \n",
    "        backrop ke sebuah single mini batch.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        temp_weights = self.weights\n",
    "        self.weights = [w + momentum * pw +(learning_rate/len(mini_batch)) * nw\n",
    "                        for w, nw, pw in zip(self.weights, nabla_w, prev_weights)]\n",
    "        temp_biases = self.biases\n",
    "        self.biases = [b + momentum *pb +  (learning_rate/len(mini_batch)) * nb\n",
    "                       for b, nb, pb in zip(self.biases, nabla_b, prev_biases)]\n",
    "        return (temp_weights, temp_biases)\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Mengembalikan nilai tuple ``(nabla_b, nabla_w)`` yang \n",
    "        merepresentasikan gradien untuk cost function C_x.  ``nabla_b`` dan\n",
    "        ``nabla_w`` adalah layer lists dari numpy arrays.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        if self.print_status :\n",
    "            print(\"FORWARD\")\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list untuk menyimpan semua activation, layer by layer\n",
    "        zs = [] # list untuk menyimpan net function, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b.transpose()[0]\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        if self.print_status :\n",
    "            print(\"BACKWARD\")\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = multiply(delta, activations[-2])\n",
    "        \n",
    "        if self.print_status :\n",
    "            print(\"delta\")\n",
    "            print(delta)\n",
    "            print(\"activation\")\n",
    "            print(activations[-2])\n",
    "            print(\"nabla_w[-1]\")\n",
    "            print(nabla_w[-1])\n",
    "            \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = multiply(delta, activations[-l-1].transpose())\n",
    "            if self.print_status :\n",
    "                print(\"delta\")\n",
    "                print(delta)\n",
    "                print(\"activation\")\n",
    "                print(activations[-l-1])\n",
    "                print(\"nabla_w[-l]\")\n",
    "                print(nabla_w[-l])\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Mengembalikan jumlah nilai prediksi benar\n",
    "        dari test data.\"\"\"\n",
    "        predicted_class = None\n",
    "        count = 0\n",
    "        for (x, y) in test_data :\n",
    "            if 2*self.feedforward(x) >= 1 : \n",
    "                predicted_class = 1 \n",
    "            else :\n",
    "                predicted_class = 0\n",
    "                \n",
    "            if predicted_class == y :\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Mengembalikan prediksi terhadap data.\"\"\"\n",
    "        predicted_class = None\n",
    "        count = 0\n",
    "        predicted = []\n",
    "        for x in test_data :\n",
    "            if 2*self.feedforward(x) >= 1 : \n",
    "                predicted_class = 1 \n",
    "            else :\n",
    "                predicted_class = 0\n",
    "            predicted.append(predicted_class)\n",
    "        return predicted\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"derivatif parsial dari cost function\"\"\"\n",
    "        return np.squeeze(y-output_activations)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"Fungsi sigmoid.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Turunan fungsi sigmoid.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def multiply(A, B):\n",
    "    result = []\n",
    "    for i in range(len(A)) :\n",
    "        row = []\n",
    "        for j in range(len(B)):\n",
    "            row.append(A[i]*B[j])\n",
    "        result.append(row)\n",
    "        \n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
