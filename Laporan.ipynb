{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laporan Tugas Besar II\n",
    "## Feed Forward Neural Network\n",
    "### IF4071 Pembelajaran Mesin\n",
    "\n",
    "\n",
    "Oleh:\n",
    "* Muhammad Hilmi Asyrofi - 13515083\n",
    "* Muhammad Rafid Amrullah - 13515125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Implementasi Classifier from Scratch\n",
    "\n",
    "Dibuat Neural Network untuk melakukan klasifikasi data weather. Neural Network merupakan fully connected layer yang memiliki jumlah hidden layer maksimal 10. Jumlah node dalam setiap hidden layer dapat bervariasi. Bagian backpropagation diimplementasikan  seperti contoh algoritma pada buku Tom Mitchell hal. 98. Neural network menggunakan fungsi aktivasi sigmoid untuk semua hidden layer maupun output layer. Node output untuk klasifikasi berjumlah 1. \n",
    "\n",
    "Program memberikan pilihan untuk menggunakan momentum atau tidak. Program mengimplementasikan mini-batch stochastic gradient descent. Prorgram Stokasti Gradien Descent diimplementasikan jenis incremental (batch-size=1) dan jenis batch (batch-size=jumlah data).\n",
    "\n",
    "Fungsi loss yang digunakan pada program yang diimplementasikan kali ini adalah MSE, yaitu:\n",
    "![MSE Lossunction](img/mse.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "~~~~~~~~~~\n",
    "Sebuah kelas yang mengimplementasikan SGD untuk \n",
    "sebuah feed forward neural network.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"List ``sizes`` berisi jumlah neuron sesuai dengan\n",
    "        urutan layer. Sebagai contoh, jika dimasukkan list [2, 3, 1]\n",
    "        maka akan digenerate 3 layer network dengan layer input berisi\n",
    "        2 neuron, hiddel layer berisi 3 neuron dan layer output 1 neuron.\n",
    "        Bias dan weight diinisialisasi secara random, menggunakan \n",
    "        distribusi Gaussian dengan mean 0, dan variance 1.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.print_status = False\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Melakukan feed forward dengan input ``a``\"\"\"\n",
    "        activation = a\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b.transpose()[0]\n",
    "            activation = sigmoid(z)\n",
    "        return activation\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, learning_rate, \n",
    "            momentum=0,test_data=None, print_status=False):\n",
    "        \"\"\"Melatih NN dengan mini-batch SGD.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.print_status = print_status\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            prev_weights = None\n",
    "            prev_biases = None\n",
    "            first = True\n",
    "            for mini_batch in mini_batches:\n",
    "                if first :\n",
    "                    prev_weights = self.weights\n",
    "                    prev_biases = self.biases\n",
    "                prev_weights, prev_biases = self.update_mini_batch(mini_batch, learning_rate, momentum, prev_weights, prev_biases)\n",
    "    #             if self.print_status :\n",
    "#             if test_data:\n",
    "#                 print(\"Epoch \" +  str(j+1))\n",
    "#                 print(\"\\tAccuracy: \" + str(100*self.evaluate(test_data)/n_test) + \"%\")\n",
    "#             else:\n",
    "#                 print(\"Epoch \" + str(j+1) + \" complete\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if test_data:\n",
    "            print(\"Accuracy: \" + str(100*self.evaluate(test_data)/n_test) + \"%\")\n",
    "            print(\"Execution time: \" + str(elapsed_time) + \"s\")\n",
    "\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, learning_rate, momentum, prev_weights, prev_biases):\n",
    "        \"\"\"Update bobot dari network dengan mengaplikasikan \n",
    "        backrop ke sebuah single mini batch.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        temp_weights = self.weights\n",
    "        self.weights = [w + momentum * pw +(learning_rate/len(mini_batch)) * nw\n",
    "                        for w, nw, pw in zip(self.weights, nabla_w, prev_weights)]\n",
    "        temp_biases = self.biases\n",
    "        self.biases = [b + momentum *pb +  (learning_rate/len(mini_batch)) * nb\n",
    "                       for b, nb, pb in zip(self.biases, nabla_b, prev_biases)]\n",
    "        return (temp_weights, temp_biases)\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Mengembalikan nilai tuple ``(nabla_b, nabla_w)`` yang \n",
    "        merepresentasikan gradien untuk cost function C_x.  ``nabla_b`` dan\n",
    "        ``nabla_w`` adalah layer lists dari numpy arrays.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        if self.print_status :\n",
    "            print(\"FORWARD\")\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list untuk menyimpan semua activation, layer by layer\n",
    "        zs = [] # list untuk menyimpan net function, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b.transpose()[0]\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        if self.print_status :\n",
    "            print(\"BACKWARD\")\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = multiply(delta, activations[-2])\n",
    "        \n",
    "        if self.print_status :\n",
    "            print(\"delta\")\n",
    "            print(delta)\n",
    "            print(\"activation\")\n",
    "            print(activations[-2])\n",
    "            print(\"nabla_w[-1]\")\n",
    "            print(nabla_w[-1])\n",
    "            \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = multiply(delta, activations[-l-1].transpose())\n",
    "            if self.print_status :\n",
    "                print(\"delta\")\n",
    "                print(delta)\n",
    "                print(\"activation\")\n",
    "                print(activations[-l-1])\n",
    "                print(\"nabla_w[-l]\")\n",
    "                print(nabla_w[-l])\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Mengembalikan jumlah nilai prediksi benar\n",
    "        dari test data.\"\"\"\n",
    "        predicted_class = None\n",
    "        count = 0\n",
    "        for (x, y) in test_data :\n",
    "            if 2*self.feedforward(x) >= 1 : \n",
    "                predicted_class = 1 \n",
    "            else :\n",
    "                predicted_class = 0\n",
    "                \n",
    "            if predicted_class == y :\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Mengembalikan prediksi terhadap data.\"\"\"\n",
    "        predicted_class = None\n",
    "        count = 0\n",
    "        predicted = []\n",
    "        for x in test_data :\n",
    "            if 2*self.feedforward(x) >= 1 : \n",
    "                predicted_class = 1 \n",
    "            else :\n",
    "                predicted_class = 0\n",
    "            predicted.append(predicted_class)\n",
    "        return predicted\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"derivatif parsial dari cost function\"\"\"\n",
    "        return np.squeeze(y-output_activations)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"Fungsi sigmoid.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Turunan fungsi sigmoid.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def multiply(A, B):\n",
    "    result = []\n",
    "    for i in range(len(A)) :\n",
    "        row = []\n",
    "        for j in range(len(B)):\n",
    "            row.append(A[i]*B[j])\n",
    "        result.append(row)\n",
    "        \n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pengujian Program dengan Dataset tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.0%\n",
      "Execution time: 0.4452085494995117s\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data[0:100], iris.target[0:100],\n",
    "    test_size=0.2, random_state=42)\n",
    "train_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
    "test_data = [(x, y) for x, y in zip(X_test, y_test)]\n",
    "\n",
    "neural_network = Network([4,5,10,1])\n",
    "neural_network.SGD(train_data, 50, 5, 0.1, momentum=0.0001, test_data=test_data)\n",
    "# neural_network.SGD(train_data, 4000, 5, 0.1, test_data, print_status=True)\n",
    "\n",
    "# check model load with test data\n",
    "# score = metrics.accuracy_score(y_test, neural_network.predict(X_test))\n",
    "# print(\"Accuracy: \" + str(score*100.0) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Implementasi Classifier dengan Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Melakukan import library yang dibutuhkan dan import dataset weather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = pd.read_csv('dataset/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan encoding data non-numerik menjadi numerik menggunakan LabelEncoder\n",
    "# Kemuadian melakukan hold-out dengan split 10%\n",
    "\n",
    "dt = iris.iloc[:, np.r_[0:1, 3:4]].apply(LabelEncoder().fit_transform)\n",
    "dt = dt.assign(temp=iris.iloc[:, 1:2])\n",
    "dt = dt.assign(humidity=iris.iloc[:, 2:3])\n",
    "cols = dt.columns.tolist()\n",
    "cols = cols[0:1] + cols[2:] + cols[1:2]\n",
    "dt = dt[cols]\n",
    "data_train = np.array(dt)\n",
    "\n",
    "iris_label = iris.iloc[:,-1]\n",
    "species_encoder = LabelEncoder().fit(iris_label)\n",
    "label_target = species_encoder.transform(iris_label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train, label_target, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat neural network dengan input layer yang tersusun dari 4 node, \n",
    "# 2 hidden layer yang masing-masing jumlah nodenya 2 dan 10, \n",
    "# dan layer output dengan jumlah node 1\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=4, activation='sigmoid', input_shape=(4,)))\n",
    "\n",
    "network.add(layers.Dense(units=2, activation='sigmoid'))\n",
    "\n",
    "network.add(layers.Dense(units=10, activation='sigmoid'))\n",
    "\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='mse',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12 samples, validate on 2 samples\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 84ms/step - loss: 0.3433 - acc: 0.4167 - val_loss: 0.5341 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 353us/step - loss: 0.3410 - acc: 0.4167 - val_loss: 0.5279 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 904us/step - loss: 0.3379 - acc: 0.4167 - val_loss: 0.5218 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 425us/step - loss: 0.3356 - acc: 0.4167 - val_loss: 0.5155 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 377us/step - loss: 0.3328 - acc: 0.4167 - val_loss: 0.5094 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3307 - acc: 0.4167 - val_loss: 0.5033 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 882us/step - loss: 0.3280 - acc: 0.4167 - val_loss: 0.4974 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3253 - acc: 0.4167 - val_loss: 0.4917 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 944us/step - loss: 0.3233 - acc: 0.4167 - val_loss: 0.4856 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3207 - acc: 0.4167 - val_loss: 0.4798 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 945us/step - loss: 0.3187 - acc: 0.4167 - val_loss: 0.4741 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 903us/step - loss: 0.3169 - acc: 0.4167 - val_loss: 0.4684 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 644us/step - loss: 0.3140 - acc: 0.4167 - val_loss: 0.4631 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 890us/step - loss: 0.3117 - acc: 0.4167 - val_loss: 0.4575 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3098 - acc: 0.4167 - val_loss: 0.4518 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 877us/step - loss: 0.3074 - acc: 0.4167 - val_loss: 0.4463 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 829us/step - loss: 0.3060 - acc: 0.4167 - val_loss: 0.4405 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 976us/step - loss: 0.3031 - acc: 0.4167 - val_loss: 0.4354 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.3013 - acc: 0.4167 - val_loss: 0.4299 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 581us/step - loss: 0.2999 - acc: 0.4167 - val_loss: 0.4243 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 901us/step - loss: 0.2980 - acc: 0.4167 - val_loss: 0.4196 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2961 - acc: 0.4167 - val_loss: 0.4152 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2946 - acc: 0.4167 - val_loss: 0.4113 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 682us/step - loss: 0.2936 - acc: 0.4167 - val_loss: 0.4073 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 963us/step - loss: 0.2918 - acc: 0.4167 - val_loss: 0.4034 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 700us/step - loss: 0.2905 - acc: 0.4167 - val_loss: 0.3994 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 633us/step - loss: 0.2889 - acc: 0.4167 - val_loss: 0.3952 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2874 - acc: 0.4167 - val_loss: 0.3911 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 803us/step - loss: 0.2863 - acc: 0.4167 - val_loss: 0.3865 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 822us/step - loss: 0.2848 - acc: 0.4167 - val_loss: 0.3822 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 645us/step - loss: 0.2834 - acc: 0.4167 - val_loss: 0.3780 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 718us/step - loss: 0.2820 - acc: 0.4167 - val_loss: 0.3740 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2806 - acc: 0.4167 - val_loss: 0.3702 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 923us/step - loss: 0.2799 - acc: 0.4167 - val_loss: 0.3663 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 710us/step - loss: 0.2784 - acc: 0.4167 - val_loss: 0.3629 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 679us/step - loss: 0.2770 - acc: 0.4167 - val_loss: 0.3593 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 812us/step - loss: 0.2762 - acc: 0.4167 - val_loss: 0.3552 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 638us/step - loss: 0.2750 - acc: 0.4167 - val_loss: 0.3513 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 567us/step - loss: 0.2738 - acc: 0.4167 - val_loss: 0.3476 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 825us/step - loss: 0.2731 - acc: 0.4167 - val_loss: 0.3440 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 571us/step - loss: 0.2717 - acc: 0.4167 - val_loss: 0.3411 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2707 - acc: 0.4167 - val_loss: 0.3382 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 737us/step - loss: 0.2697 - acc: 0.4167 - val_loss: 0.3347 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 824us/step - loss: 0.2689 - acc: 0.4167 - val_loss: 0.3308 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2679 - acc: 0.4167 - val_loss: 0.3267 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 708us/step - loss: 0.2664 - acc: 0.4167 - val_loss: 0.3230 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 631us/step - loss: 0.2658 - acc: 0.4167 - val_loss: 0.3190 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 950us/step - loss: 0.2648 - acc: 0.4167 - val_loss: 0.3153 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 940us/step - loss: 0.2639 - acc: 0.4167 - val_loss: 0.3119 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 872us/step - loss: 0.2630 - acc: 0.4167 - val_loss: 0.3088 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Melakukan pembelajaran pada neural network yang telah dibuat. \n",
    "# Dari hasil pembelajaran, akurasi test selalu 100%, dapat dilihat dari val_acc yang selalu bernilai 1 (100%)\n",
    "\n",
    "training = network.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=50, \n",
    "                      verbose=1, \n",
    "                      batch_size=5,\n",
    "                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perbandingan Kinerja Classifier A dan B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan encoding data non-numerik menjadi numerik menggunakan LabelEncoder\n",
    "# Kemuadian melakukan hold-out dengan split 10%\n",
    "\n",
    "dt = iris.iloc[:, np.r_[0:1, 3:4]].apply(LabelEncoder().fit_transform)\n",
    "dt = dt.assign(temp=iris.iloc[:, 1:2])\n",
    "dt = dt.assign(humidity=iris.iloc[:, 2:3])\n",
    "cols = dt.columns.tolist()\n",
    "cols = cols[0:1] + cols[2:] + cols[1:2]\n",
    "dt = dt[cols]\n",
    "data_train = np.array(dt)\n",
    "\n",
    "iris_label = iris.iloc[:,-1]\n",
    "species_encoder = LabelEncoder().fit(iris_label)\n",
    "label_target = species_encoder.transform(iris_label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train, label_target, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinerja Classifier 1.a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n",
      "Execution time: 0.594524621963501s\n"
     ]
    }
   ],
   "source": [
    "neural_network = Network([4,5,10,1])\n",
    "\n",
    "start_time = time.time()\n",
    "neural_network.SGD(train_data, 50, 1, 0.1, momentum=0.00001)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, neural_network.predict(X_test))\n",
    "print(\"Accuracy: \" + str(accuracy*100.0) + \"%\")\n",
    "print(\"Execution time: \" + str(elapsed_time) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-size=n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n",
      "Execution time: 0.9152624607086182s\n"
     ]
    }
   ],
   "source": [
    "neural_network = Network([4,5,10,1])\n",
    "\n",
    "start_time = time.time()\n",
    "neural_network.SGD(train_data, len(train_data), 1, 0.1, momentum=0.00001)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, neural_network.predict(X_test))\n",
    "print(\"Accuracy: \" + str(accuracy*100.0) + \"%\")\n",
    "print(\"Execution time: \" + str(elapsed_time) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinerja Classifier 1.b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 71\n",
      "Trainable params: 71\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 124us/step\n",
      "Accuracy: 100.0%\n",
      "Execution time: 0.3622128963470459s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "network.summary()\n",
    "training = network.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=50, \n",
    "                      verbose=0, \n",
    "                      batch_size=1)\n",
    "\n",
    "scores = network.evaluate(X_test, y_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Accuracy: \" + str(scores[1]*100.0) + \"%\")\n",
    "print(\"Execution time: \" + str(elapsed_time) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch-size=n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 71\n",
      "Trainable params: 71\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 0s 127us/step\n",
      "Accuracy: 100.0%\n",
      "Execution time: 0.04487276077270508s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "network.summary()\n",
    "training = network.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs=50, \n",
    "                      verbose=0, \n",
    "                      batch_size=len(X_train))\n",
    "\n",
    "scores = network.evaluate(X_test, y_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Accuracy: \" + str(scores[1]*100.0) + \"%\")\n",
    "print(\"Execution time: \" + str(elapsed_time) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Perbandingan Kinerja\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan beberapa hasil di atas diperoleh tabel sebagai berikut. \n",
    "\n",
    "| Classifier | Batch-size | Accuracy | Execution time         \n",
    "| :- |-------------: | ---------: | :-:\n",
    "|Classifier 1.a.| 1 | 100%| 0.5934426784515381s\n",
    "|Classifier 1.a.| Jumlah data|100% | 0.937950849533081s\n",
    "|Classifier 1.b.| 1|100% | 0.3534877300262451s\n",
    "|Classifier 1.b.| Jumlah data|100% | 0.0456850528717041s\n",
    "\n",
    "Semua jenis Classifier dengan variasi batch-sizenya telah dilakukan pengujian kinerja dan menunjukkan hasil yang bagus, yaitu memiliki akurasi 100%. Berdasarkan tabel tersebut, dapat diperoleh informasi bahwa waktu ekseksusi yang dibutuhkan classifier 1.b cenderung lebih cepat dibandingkan dengan waktu yang diperlukan oleh Classifier 1.a. Hal ini diduga karena Classifier 1.b. merupakan library yang dikembangkan oleh para pakar sehingga telah dilakukan optimasi. Disamping itu, waktu yang dibutuhkan pada Classifier 1.b dengan batch-size=n cenderung lebih cepat karena diduga jumlah peng-update-an yang dilakukan jauh lebih sedikit dibandingkan dengan batch-size=1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembagian Tugas\n",
    "\n",
    "* 13515083 Muhammad Hilmi Asyrofi - Implementasi 1.a. Classifier from Scratch dan Analisis Perbandingan Kinerja\n",
    "* 13515125 Muhammad Rafid Amrullah - Implementasi 1.b. Classifier dengan Keras\n",
    "\n",
    "Dokumentasi pengerjaan tugas besar ini dapat dilihat pada repositori github berikut:\n",
    "https://github.com/mhilmiasyrofi/feed-forward-neural-network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
